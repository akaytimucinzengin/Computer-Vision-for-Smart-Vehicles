# scripts/create_submission.py
#
# This is the final step in the pipeline. It takes the raw prediction files
# generated by the model and formats them into a single, human-readable CSV file.
# This is a crucial step for analyzing the model's performance on the test set.
#
# The script works by:
# 1.  Finding the most recent prediction directory created by the evaluation script.
# 2.  Loading the class names from the `data.yaml` file to map class IDs (e.g., 0, 1, 2)
#     back to their human-readable names (e.g., 'car', 'traffic sign').
# 3.  Iterating through each raw prediction .txt file. Each file corresponds to a
#     test image and contains one line for each detected object.
# 4.  Parsing the class ID and bounding box coordinates for each detection.
# 5.  Aggregating all detections into a single list.
# 6.  Converting the list into a pandas DataFrame and saving it as `results.csv`.
#
# This approach is fully automated and does not depend on any template files,
# making it adaptable to any test set.

import pandas as pd
import os
import yaml

def get_latest_predict_dir():
    """Finds the directory of the most recent prediction run."""
    runs_dir = 'runs/detect'
    if not os.path.exists(runs_dir):
        return None
    
    # List all prediction directories (e.g., 'predict', 'predict2', ...)
    predict_dirs = [os.path.join(runs_dir, d) for d in os.listdir(runs_dir) if d.startswith('predict')]
    if not predict_dirs:
        return None
        
    # Find the most recently modified prediction directory
    latest_predict_dir = max(predict_dirs, key=os.path.getmtime)
    labels_dir = os.path.join(latest_predict_dir, 'labels')
    
    if os.path.exists(labels_dir):
        return labels_dir
    else:
        return None

def create_submission():
    """
    Creates a CSV submission file from the model's raw prediction files.
    
    This function automates the process of converting the YOLO prediction
    output into a structured CSV file for easy analysis. It is designed to
    be self-contained, generating the results without needing a template.
    
    The process is as follows:
    1.  **Find Predictions**: It locates the directory containing the .txt
        prediction files from the most recent evaluation run.
    2.  **Load Class Names**: It reads the `data.yaml` file to get the mapping
        from class IDs to class names.
    3.  **Parse Predictions**: It iterates through each .txt file, parsing
        each detected object's class ID and bounding box coordinates.
    4.  **Aggregate and Format**: It compiles the parsed data into a list of
        dictionaries, which is then converted into a pandas DataFrame.
    5.  **Save Results**: The final DataFrame is saved to `results.csv`.
    """
    # Find the directory with the latest prediction .txt files.
    predictions_dir = get_latest_predict_dir()
    if predictions_dir is None:
        print("Error: No prediction directory found. Please run the evaluation script first.")
        return

    # Load class names from the data.yaml file.
    yaml_path = 'data/data.yaml'
    if not os.path.exists(yaml_path):
        print(f"Error: {yaml_path} not found. Cannot map class IDs to names.")
        return
    with open(yaml_path, 'r') as f:
        data_yaml = yaml.safe_load(f)
    class_names = data_yaml['names']

    # Process each prediction file and aggregate the results.
    all_predictions = []
    for label_file in os.listdir(predictions_dir):
        img_name = label_file.replace('.txt', '.jpg')  # Assume .jpg, but could be other formats

        with open(os.path.join(predictions_dir, label_file), 'r') as f:
            for line in f:
                parts = line.strip().split()
                if len(parts) == 5:
                    class_id, x_center, y_center, width, height = parts
                    
                    # Convert class ID to name
                    class_name = class_names[int(class_id)]
                    
                    all_predictions.append({
                        'image_name': img_name,
                        'class': class_name,
                        'x_center': float(x_center),
                        'y_center': float(y_center),
                        'width': float(width),
                        'height': float(height)
                    })

    if not all_predictions:
        print("No predictions found to process.")
        return

    # Convert the list of predictions to a pandas DataFrame.
    submission_df = pd.DataFrame(all_predictions)

    # Save the updated submission DataFrame to a new CSV file.
    output_path = "results.csv"
    submission_df.to_csv(output_path, index=False)
    print(f"Prediction results saved to: {output_path}")

if __name__ == '__main__':
    create_submission()
